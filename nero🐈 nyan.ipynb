{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NystroüêàNyan Nyora [WIP?]\n",
    "\n",
    "\n",
    "The work is based on\n",
    "* idea of LoRA (take little parameters to represent big matrix)\n",
    "* idea of Nystr√∂mformer and Nystr√∂m approximation (take 1 matrix, beat it into smaller parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(PLACEHOLDER FOR A KITTEN OR A CATGIRL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports,  let's skip this part, it's not my favorite\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "from pathlib import Path  #(only place where it will be used )\n",
    "\n",
    "MODEL_PATH = Path(\"~/models/ahxt_LiteLlama-460M-1T\").expanduser()\n",
    "DTYPE = torch.bfloat16\n",
    "ATTN_IMPL=\"flash_attention_2\"\n",
    "DEVICE = \"cuda\"\n",
    "RANK=32\n",
    "BATCH_SIZE=5\n",
    "N_CTX=640\n",
    "DATASET = \"iohadrubin/wikitext-103-raw-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPOILER: In the end I've found that LiteLlama tokenizer eats double quote '\"'...\n",
    "\n",
    "(https://huggingface.co/ahxt/LiteLlama-460M-1T/discussions/9#659f6f894c074ce5e4e9532c)\n",
    "\n",
    "... as it thinks it's a [PAD], so results are questionable, need to check on other model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build Nystronyan block that uses less than (NxN) parameters to be \n",
    "# To read inspration, check https://arxiv.org/pdf/2102.03902.pdf, however we need only a single formula from there\n",
    "# as we don't care that much of finer details such as \"optimization\"\n",
    "class NystroNyan(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, r: int):\n",
    "        \n",
    "        super().__init__()\n",
    "        # mirror nn.Linear reports\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = r\n",
    "\n",
    "        # big matrix n x n, split into four parts\n",
    "        #   \n",
    "        #  (r r)          (r, n_col-r) \n",
    "        #    A                  B\n",
    "        #    F                  C\n",
    "        #  (n_row-r r)  (n_row-r n_col-r)\n",
    "        # our part of interest  is C.\n",
    "\n",
    "        self.A = nn.Parameter(torch.rand(r, r))\n",
    "        self.B = nn.Parameter(torch.rand(r, out_features-r))\n",
    "        self.F = nn.Parameter(torch.rand(in_features-r, r))\n",
    "        \n",
    "    @property\n",
    "    def A_(self):\n",
    "        # pinverse doesn't support bf16\n",
    "        return self.A.float().pinverse().to(dtype=self.A.dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we need Moore-Penrose inverse\n",
    "        A_ = self.A_\n",
    "\n",
    "        AB = torch.cat((self.A, self.B), -1)  # AB upper half\n",
    "        AF = torch.cat((self.A, self.F), 0)   # AF left half\n",
    "        \n",
    "        reconsturcted_matrix = AF @ A_ @ AB\n",
    "\n",
    "        return x @ reconsturcted_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(n=4096, r=8)\n",
      "NystroNyan:\t 65472\n",
      "LoRA:\t 65536\n",
      "\n",
      "(n=4096, r=16)\n",
      "NystroNyan:\t 130816\n",
      "LoRA:\t 131072\n",
      "\n",
      "(n=4096, r=32)\n",
      "NystroNyan:\t 261120\n",
      "LoRA:\t 262144\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "def model_numel(m, grad_only=False):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad or not grad_only)\n",
    "    \n",
    "def sanity_check(n=4096, r=32):\n",
    "    print(f\"\\n({n=}, {r=})\")\n",
    "    print(\"NystroNyan:\\t\", model_numel(NystroNyan(n, n, r)))\n",
    "    print(\"LoRA:\\t\", torch.zeros(2, n, r).numel())\n",
    "\n",
    "sanity_check(r=8)\n",
    "sanity_check(r=16)\n",
    "sanity_check(r=32)\n",
    "# We are more parameter efficient. Nyaa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(50304, 1024, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaFlashAttention2(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=128, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=128, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading a model\n",
    "def reset_model():\n",
    "    global model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=DTYPE,\n",
    "        attn_implementation=ATTN_IMPL\n",
    "    ).to(device=DEVICE)\n",
    "    return model\n",
    "reset_model()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "if tokenizer.pad_token_id is None:    \n",
    "    tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "tokenizer_path = MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data loading\n",
    "def tokenize_text(samples, tokenizer):\n",
    "    return {\"input_ids\": tokenizer(samples[\"text\"]).input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data loading\n",
    "def get_dl(batch_size, split=\"train\"):\n",
    "    ds = datasets.load_dataset(DATASET)\n",
    "    ds = ds[split]\n",
    "    assert \"text\" in ds.column_names\n",
    "    ds = ds.map(\n",
    "        tokenize_text,\n",
    "        remove_columns=ds.column_names,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "        batched=True)\n",
    "    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    dl = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collator,\n",
    "        shuffle=False)\n",
    "    return dl\n",
    "\n",
    "import functools\n",
    "\n",
    "# We cache result to omit stupid messages like \"Found cached dataset parquet\"\n",
    "@functools.cache\n",
    "def get_dl_cached(batch_size, split=\"train\"):\n",
    "    return get_dl(batch_size, split)\n",
    "\n",
    "def iter_batch(batch: dict, n_ctx: int):\n",
    "    for start in range(0, batch.input_ids.shape[1], n_ctx):\n",
    "        part = {k: v[:, start:start+n_ctx].to(DEVICE) for k,v in batch.items()}\n",
    "        part[\"progress\"] = (start, batch.input_ids.shape[1])\n",
    "        yield part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need a module for injecting our cute kitten\n",
    "class NyanInjector(nn.Module):\n",
    "    def __init__(self, base: nn.Linear, r: int) -> None:\n",
    "        super().__init__()\n",
    "        assert isinstance(base, nn.Linear)\n",
    "        self.base = base\n",
    "        self.delta = NystroNyan(base.in_features, base.out_features, r)\n",
    "        for p in self.delta.parameters():\n",
    "            p.data *= model.config.initializer_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.base(x)\n",
    "        y = y + self.delta(x)\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze everything ‚ùÑ\n",
    "model.requires_grad_(False)\n",
    "\n",
    "# Hot catgirls go in üî•\n",
    "for layer in model.model.layers:\n",
    "    attn = layer.self_attn\n",
    "    attn.q_proj = NyanInjector(attn.q_proj, r=RANK).to(dtype=DTYPE, device=DEVICE)\n",
    "    attn.v_proj = NyanInjector(attn.v_proj, r=RANK).to(dtype=DTYPE, device=DEVICE)\n",
    "\n",
    "parms_to_train = model_numel(model, grad_only=True)\n",
    "parms_total = model_numel(model, grad_only=False) \n",
    "print(f\"#parms: {parms_total}, trainable: {parms_to_train} ({parms_to_train/parms_total:%})\")\n",
    "\n",
    "# In PEFT LoRA: trainable params: 2457600 || all params: 464143360 || trainable%: 0.529491577774591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url='https://huggingface.co/datasets/iohadrubin/wikitext-103-raw-v1/resolve/main/dataset_infos.json'\n",
      "cached_url='https://huggingface.co/datasets/iohadrubin/wikitext-103-raw-v1/resolve/main/dataset_infos.json', cache_path='/home/fella/.cache/huggingface/datasets/downloads/fd88374720898676c1aac99c1637230812afd06875ab0daf8df9c2e940e5eb56.ef59c73aaa8d4724466db27d4c6ee30e773d72f4a9ad4791ce2a85c70acc8e72'\n",
      "url='https://huggingface.co/datasets/iohadrubin/wikitext-103-raw-v1/resolve/main/README.md'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/fella/.cache/huggingface/datasets/iohadrubin___parquet/iohadrubin--wikitext-103-raw-v1-a0b8bc67accf87c3/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not connected, checking cache `/home/fella/.cache/huggingface/datasets/downloads/b03c45f13f201de25b7a5863a2ac945f80be7ca141c039f519075e10bda67e2c for https://huggingface.co/datasets/iohadrubin/wikitext-103-raw-v1/resolve/main/README.md`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfebbb87476543698a39bd9a5b4895ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/fella/.cache/huggingface/datasets/iohadrubin___parquet/iohadrubin--wikitext-103-raw-v1-a0b8bc67accf87c3/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-f17f8ff98091523f.arrow\n"
     ]
    }
   ],
   "source": [
    "dl = get_dl(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1400a36e0d4e58b1a3ed1bc58d3ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = torch.optim.AdamW(model.parameters(), fused=True)\n",
    "\n",
    "for ib, b in enumerate(bar := tqdm(dl)):\n",
    "    if ib < 101:\n",
    "        continue\n",
    "    for mb in iter_batch(b, N_CTX):\n",
    "        (cur, total) = mb.pop(\"progress\")\n",
    "        loss = model(**mb).loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        bar.set_description(f'L:{loss.item():.4} P:{cur/total:%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload nyora\n",
    "for layer in model.model.layers:\n",
    "    attn = layer.self_attn\n",
    "    assert isinstance(attn.q_proj, NyanInjector)\n",
    "    attn.q_proj = attn.q_proj.base\n",
    "    attn.v_proj = attn.v_proj.base\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2457600 || all params: 464143360 || trainable%: 0.529491577774591\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, \n",
    "                         inference_mode=False, r=RANK, \n",
    "                         lora_alpha=32, \n",
    "                         lora_dropout=0.1, \n",
    "                         target_modules=['q_proj', 'v_proj'])\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1063f96fb5e04da8899e3df02d328e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3.025418281555176"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def run_valid():\n",
    "    dl = get_dl_cached(batch_size=1, split=\"validation\")\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for _, b in enumerate(bar := tqdm(dl)):\n",
    "        for mb in iter_batch(b, N_CTX):\n",
    "            (cur, total) = mb.pop(\"progress\")\n",
    "            loss = model(**mb).loss.item()\n",
    "            bar.set_description(f'L:{loss:.4} P:{cur/total:%}')\n",
    "            losses.append(loss)\n",
    "    model.train()\n",
    "    return torch.tensor(losses).mean().item()\n",
    "            \n",
    "\n",
    "run_valid()\n",
    "# Vanila: 3.132427930831909\n",
    "\n",
    "# Nyora  v \n",
    "#  100b5: 3.131727695465088\n",
    "# 1295b5: 3.033968448638916\n",
    "# 1950b5: 3.055903434753418\n",
    "# 2450b5: 3.0657265186309814\n",
    "# 5914b5: 3.0219664573669434\n",
    "\n",
    "# Lora\n",
    "# 0(no train): 3.132427930831909\n",
    "#  101b5: 3.0502851009368896\n",
    "# 5914b5: 3.025418281555176\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/home/fella/models/ahxt_LiteLlama-460M-1T\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50304\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in get_dl_cached(split=\"validation\", batch_size=1):\n",
    "    print(model.config)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7556, device='cuda:0') >Sm<\n",
      "tensor(4386, device='cuda:0') >iling<\n",
      "tensor(11, device='cuda:0') >,<\n",
      "tensor(25849, device='cuda:0') > waving<\n",
      "tensor(284, device='cuda:0') > to<\n",
      "tensor(262, device='cuda:0') > the<\n",
      "tensor(18662, device='cuda:0') > rhythm<\n",
      "tensor(286, device='cuda:0') > of<\n",
      "tensor(262, device='cuda:0') > the<\n",
      "tensor(2647, device='cuda:0') > music<\n",
      "tensor(11, device='cuda:0') >,<\n",
      "tensor(531, device='cuda:0') > said<\n",
      "tensor(284, device='cuda:0') > to<\n",
      "tensor(683, device='cuda:0') > him<\n",
      "tensor(691, device='cuda:0') > only<\n",
      "tensor(1573, device='cuda:0') > word<\n",
      "tensor(25, device='cuda:0') >:<\n",
      "tensor(198, device='cuda:0') >\n",
      "<\n",
      "tensor(198, device='cuda:0') >\n",
      "<\n",
      "tensor(1, device='cuda:0') >\"<\n",
      "tensor(1858, device='cuda:0') >There<\n",
      "tensor(318, device='cuda:0') > is<\n",
      "tensor(257, device='cuda:0') > a<\n",
      "tensor(2415, device='cuda:0') > woman<\n",
      "tensor(994, device='cuda:0') > here<\n",
      "tensor(9975, device='cuda:0') > tonight<\n",
      "tensor(553, device='cuda:0') >,\"<\n",
      "tensor(339, device='cuda:0') > he<\n",
      "tensor(531, device='cuda:0') > said<\n",
      "tensor(845, device='cuda:0') > very<\n",
      "tensor(12703, device='cuda:0') > quietly<\n",
      "tensor(13, device='cuda:0') >.<\n",
      "tensor(220, device='cuda:0') > <\n",
      "tensor(220, device='cuda:0') > <\n",
      "tensor(198, device='cuda:0') >\n",
      "<\n",
      "tensor(1, device='cuda:0') >\"<\n",
      "tensor(1026, device='cuda:0') >It<\n",
      "Smiling, waving to the rhythm of the music, said to him only word:\n",
      "\n",
      "\"There is a woman here tonight,\" he said very quietly.  \n",
      "\"It\n"
     ]
    }
   ],
   "source": [
    "def sanity_prompt(p):\n",
    "    x = tokenizer(p, return_tensors=\"pt\").to(DEVICE)\n",
    "    #del x[\"token_type_ids\"]\n",
    "    x = model.generate(**x, max_new_tokens=20, do_sample=True).ravel()\n",
    "    x2 = tokenizer.decode(x)\n",
    "    return x,x2\n",
    "x1, x2 = sanity_prompt(\"Smiling, waving to the rhythm of the music, said to him only word:\")\n",
    "for x in x1:\n",
    "    print(x, f'>'+tokenizer.decode(x)+'<')\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(\n",
       "  in_features=1024, out_features=1024, bias=False\n",
       "  (lora_dropout): ModuleDict(\n",
       "    (default): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lora_A): ModuleDict(\n",
       "    (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "  )\n",
       "  (lora_B): ModuleDict(\n",
       "    (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "  )\n",
       "  (lora_embedding_A): ParameterDict()\n",
       "  (lora_embedding_B): ParameterDict()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.model.layers[1].self_attn.q_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[[31373,     0, 23748,     0]]]), 'attention_mask': tensor([[[1, 1, 1, 1]]]), 'labels': tensor([[[31373,  -100, 23748,  -100]]])}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token_id = 0\n",
    "collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "features = tokenizer('hello! hello!', return_tensors=\"pt\")\n",
    "collator([features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2(\"!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
