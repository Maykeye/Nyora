{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NystroüêàNyan Nyora [WIP?]\n",
    "\n",
    "\n",
    "The work is based on\n",
    "* idea of LoRA (take little parameters to represent big matrix)\n",
    "* idea of Nystr√∂mformer and Nystr√∂m approximation (take 1 matrix, beat it into smaller parts)\n",
    "\n",
    "SPOILER: Pretty shitty result once I used real model, oh well, worth trying.\n",
    "\n",
    "All hope abandon ye who enter here further, I'ma keeping it in case which likely not to happen or to base other impls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(PLACEHOLDER FOR A KITTEN OR A CATGIRL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports,  let's skip this part, it's not my favorite\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "from pathlib import Path  #(only place where it will be used )\n",
    "\n",
    "MODEL_PATH = Path(\"~/models/TinyLlama_TinyLlama-1.1B-intermediate-step-1431k-3T\").expanduser()\n",
    "DTYPE = torch.bfloat16\n",
    "ATTN_IMPL=\"flash_attention_2\"\n",
    "DEVICE = \"cuda\"\n",
    "RANK=32\n",
    "BATCH_SIZE=3\n",
    "N_CTX=640\n",
    "DATASET = \"boolq\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPOILER: In the end I've found that LiteLlama tokenizer eats double quote '\"'...\n",
    "\n",
    "(https://huggingface.co/ahxt/LiteLlama-460M-1T/discussions/9#659f6f894c074ce5e4e9532c)\n",
    "\n",
    "... as it thinks it's a [PAD], so results are questionable, need to check on other model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build Nystronyan block that uses less than (NxN) parameters to be \n",
    "# To read inspration, check https://arxiv.org/pdf/2102.03902.pdf, however we need only a single formula from there\n",
    "# as we don't care that much of finer details such as \"optimization\"\n",
    "class NystroNyan(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, r: int):\n",
    "        \n",
    "        super().__init__()\n",
    "        # mirror nn.Linear reports\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = r\n",
    "\n",
    "        # big matrix n x n, split into four parts\n",
    "        #   \n",
    "        #  (r r)          (r, n_col-r) \n",
    "        #    A                  B\n",
    "        #    F                  C\n",
    "        #  (n_row-r r)  (n_row-r n_col-r)\n",
    "        # our part of interest  is C.\n",
    "\n",
    "        self.A = nn.Parameter(torch.rand(r, r))\n",
    "        self.B = nn.Parameter(torch.rand(r, out_features-r))\n",
    "        self.F = nn.Parameter(torch.rand(in_features-r, r))\n",
    "        \n",
    "    @property\n",
    "    def A_(self):\n",
    "        # pinverse doesn't support bf16\n",
    "        return self.A.float().pinverse().to(dtype=self.A.dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we need Moore-Penrose inverse\n",
    "        A_ = self.A_\n",
    "\n",
    "        AB = torch.cat((self.A, self.B), -1)  # AB upper half\n",
    "        AF = torch.cat((self.A, self.F), 0)   # AF left half\n",
    "        \n",
    "        reconsturcted_matrix = AF @ A_ @ AB\n",
    "\n",
    "        return x @ reconsturcted_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(n=4096, r=8)\n",
      "NystroNyan:\t 65472\n",
      "LoRA:\t 65536\n",
      "\n",
      "(n=4096, r=16)\n",
      "NystroNyan:\t 130816\n",
      "LoRA:\t 131072\n",
      "\n",
      "(n=4096, r=32)\n",
      "NystroNyan:\t 261120\n",
      "LoRA:\t 262144\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "def model_numel(m, grad_only=False):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad or not grad_only)\n",
    "    \n",
    "def sanity_check(n=4096, r=32):\n",
    "    print(f\"\\n({n=}, {r=})\")\n",
    "    print(\"NystroNyan:\\t\", model_numel(NystroNyan(n, n, r)))\n",
    "    print(\"LoRA:\\t\", torch.zeros(2, n, r).numel())\n",
    "\n",
    "sanity_check(r=8)\n",
    "sanity_check(r=16)\n",
    "sanity_check(r=32)\n",
    "# We are more parameter efficient. Nyaa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading a model\n",
    "def reset_model():\n",
    "    global model\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=DTYPE\n",
    "    )   \n",
    "     \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,        \n",
    "        torch_dtype=DTYPE,\n",
    "        attn_implementation=ATTN_IMPL,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    #model.to(device=DEVICE)\n",
    "    return model\n",
    "reset_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "if tokenizer.pad_token_id is None:    \n",
    "    tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "tokenizer_path = MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data loading\n",
    "def tokenize_text(samples, tokenizer):\n",
    "    return {\"input_ids\": tokenizer(samples[\"text\"]).input_ids}\n",
    "\n",
    "def tokenize_boolq(samples, tokenizer):\n",
    "    n_samples = len(samples[\"question\"])\n",
    "    prompt = \"\\n\\n\\n### Passage:\\n{0}\\n\\n### Question:\\n{1}\\n\\n### Answer:\\n{2}\"\n",
    "    preprompt = \"\".join([\n",
    "        prompt.format(\"Stockholm is the capital of Sweden\", \"Is Stockholm the capital of Sweden\", \"True\"),\n",
    "        prompt.format(\"3/5 is greater than -5/3\", \"Is -5/3 greater than 3/5\", \"False\")\n",
    "    ])\n",
    "    data = []\n",
    "    for i in range(n_samples):\n",
    "        passage, question, answer = samples[\"passage\"][i], samples[\"question\"][i], samples[\"answer\"][i]\n",
    "        passage = passage[:1000]\n",
    "        new_sample = preprompt + prompt.format(passage, question, answer)\n",
    "        data.append(new_sample)\n",
    "    return {\"input_ids\": tokenizer(data, return_attention_mask=False).input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data loading\n",
    "def get_dl(batch_size, split=\"train\"):\n",
    "    ds = datasets.load_dataset(DATASET)\n",
    "    ds = ds[split]    \n",
    "    ds = ds.map(\n",
    "        tokenize_boolq,\n",
    "        remove_columns=ds.column_names,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "        batched=True)\n",
    "    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    dl = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collator,\n",
    "        shuffle=False)\n",
    "    return dl\n",
    "\n",
    "import functools\n",
    "\n",
    "# We cache result to omit stupid messages like \"Found cached dataset parquet\"\n",
    "@functools.cache\n",
    "def get_dl_cached(batch_size, split=\"train\"):\n",
    "    return get_dl(batch_size, split)\n",
    "\n",
    "def iter_batch(batch: dict, n_ctx: int):\n",
    "    for start in range(0, batch.input_ids.shape[1], n_ctx):\n",
    "        part = {k: v[:, start:start+n_ctx].to(DEVICE) for k,v in batch.items()}\n",
    "        part[\"progress\"] = (start, batch.input_ids.shape[1])\n",
    "        yield part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need a module for injecting our cute kitten\n",
    "class NyanInjector(nn.Module):\n",
    "    def __init__(self, base: nn.Linear, r: int) -> None:\n",
    "        super().__init__()\n",
    "        assert isinstance(base, nn.Linear)\n",
    "        self.base = base\n",
    "        self.delta = NystroNyan(base.in_features, base.out_features, r)\n",
    "        for p in self.delta.parameters():\n",
    "            p.data *= model.config.initializer_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.base(x)\n",
    "        y = y + self.delta(x)\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#parms: 620066816, trainable: 4460544 (0.719365%)\n"
     ]
    }
   ],
   "source": [
    "# Freeze everything ‚ùÑ\n",
    "model.requires_grad_(False)\n",
    "\n",
    "# Hot catgirls go in üî•\n",
    "for layer in model.model.layers:\n",
    "    attn = layer.self_attn\n",
    "    assert hasattr(attn, \"q_proj\")\n",
    "    assert hasattr(attn, \"v_proj\")\n",
    "    attn.q_proj = NyanInjector(attn.q_proj, r=RANK).to(dtype=DTYPE, device=DEVICE)\n",
    "    attn.v_proj = NyanInjector(attn.v_proj, r=RANK).to(dtype=DTYPE, device=DEVICE)\n",
    "\n",
    "parms_to_train = model_numel(model, grad_only=True)\n",
    "parms_total = model_numel(model, grad_only=False) \n",
    "print(f\"#parms: {parms_total}, trainable: {parms_to_train} ({parms_to_train/parms_total:%})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nyan_dict():\n",
    "    all_dict = model.state_dict()\n",
    "    to_delete = [k for k in all_dict.keys() if \".delta.\" not in k]\n",
    "    for k in to_delete:\n",
    "        del all_dict[k]\n",
    "    return all_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nyan_dict(), \"weights/phi.nyan-4bit.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = get_dl(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cfbc3ef61148df89c12f61f5a4707a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "for ib, b in enumerate(bar := tqdm(dl)):\n",
    "    for mb in iter_batch(b, N_CTX):\n",
    "        (cur, total) = mb.pop(\"progress\")\n",
    "        loss = model(**mb).loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        bar.set_description(f'L:{loss.item():.4} P:{cur/total:%}')\n",
    "    #if ib % 100 == 0:\n",
    "    #    torch.save(nyan_dict(), f\"weights/ollama3.nyan-0.4bit-E{ib}.bin\")\n",
    "#torch.save(nyan_dict(), f\"weights/ollama3.nyan-0.4bit-E{ib}.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload nyora\n",
    "for layer in model.model.layers:\n",
    "    attn = layer.self_attn\n",
    "    assert isinstance(attn.q_proj, NyanInjector)\n",
    "    attn.q_proj = attn.q_proj.base\n",
    "    attn.v_proj = attn.v_proj.base\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4505600 || all params: 620111872 || trainable%: 0.7265785745188894\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, \n",
    "                         inference_mode=False, r=RANK, \n",
    "                         lora_alpha=32, \n",
    "                         lora_dropout=0.1, \n",
    "                         target_modules=['q_proj', 'v_proj'])\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79971343d4e740f7be2560bef9689c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2359/3270=72.140670% CT:1575 CF:784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7214, device='cuda:0')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def run_valid_acc(strict=False):\n",
    "    assert strict == False, \"text sampling NYI\"\n",
    "    t_true = tokenizer.encode(\"\\nTrue\", add_special_tokens=False)\n",
    "    t_true = t_true[-1]\n",
    "    t_false = tokenizer.encode(\"\\nFalse\", add_special_tokens=False)\n",
    "    t_false = t_false[-1]\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    ds = datasets.load_dataset(DATASET, split=\"validation\")\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    correct_trues = 0\n",
    "    correct_falses = 0\n",
    "    # todo: generate/w max_new_tokens=1?\n",
    "    for i in tqdm(range(len(ds))):\n",
    "        total += 1\n",
    "        samples = ds[i:i+1]\n",
    "        inputs = tokenize_boolq(samples, tokenizer)\n",
    "\n",
    "        y_true = inputs[\"input_ids\"][0].pop()\n",
    "        assert y_true in [t_true, t_false], f\"{y_true} != {t_true}/{t_false}\"\n",
    "        y_true = y_true == t_true\n",
    "        inputs = torch.tensor(inputs[\"input_ids\"]).to(device=DEVICE)\n",
    "        out = model(input_ids=inputs).logits[0, -1]\n",
    "        out = nn.functional.softmax(out, -1)\n",
    "        if not strict:\n",
    "            y_pred = out[t_true] > out[t_false]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Sttrict acc mode nyi\")\n",
    "        correct += y_true == y_pred\n",
    "        if y_true == y_pred:\n",
    "            if y_true:\n",
    "                correct_trues += 1\n",
    "            else:\n",
    "                correct_falses += 1\n",
    "        \n",
    "\n",
    "    #model.train()\n",
    "    print(f\"{correct}/{total}={correct/total:%} CT:{correct_trues} CF:{correct_falses}\")\n",
    "    return correct/total\n",
    "            \n",
    "\n",
    "run_valid_acc()\n",
    "\n",
    "# Vanilla: \n",
    "# Raw    : 1389/3270=42.477062%\n",
    "# Nyaron : 1822/3270=55.718654%\n",
    "# LoRA   : 2359/3270=72.140670%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in get_dl_cached(split=\"validation\", batch_size=1):\n",
    "    print(model.config)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> A tabby cat sits on the wall, looking at the camera.\n",
      "After a few seconds, the cat jumps down and runs to the other\n"
     ]
    }
   ],
   "source": [
    "def sanity_prompt(p):\n",
    "    x = tokenizer(p, return_tensors=\"pt\").to(DEVICE)\n",
    "    #del x[\"token_type_ids\"]\n",
    "    x = model.generate(**x, max_new_tokens=20, do_sample=False).ravel()\n",
    "    x2 = tokenizer.decode(x)\n",
    "    return x,x2\n",
    "x1, x2 = sanity_prompt(\"\"\"A tabby cat sits on the wall, looking\"\"\")\n",
    "print(x2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
